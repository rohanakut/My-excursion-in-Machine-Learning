Filter selection criteria for cnn:
https://www.researchgate.net/post/How_do_we_choose_the_filters_for_the_convolutional_layer_of_a_Convolution_Neural_Network_CNN

 Local Response Normalization:
 https://prateekvjoshi.com/2016/04/05/what-is-local-response-normalization-in-convolutional-neural-networks/


Basic implementation of cnn and ann on mnist
https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/

One-hot endcoding:
https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/

Why does batch normalistaion work?
https://www.youtube.com/watch?v=nUUqwaxLnWs

Why is RELU preferred over sigmoid?
https://datascience.stackexchange.com/questions/15484/sigmoid-vs-relu-function-in-convnets
https://stats.stackexchange.com/questions/218752/relu-vs-sigmoid-vs-softmax-as-hidden-layer-neurons

